{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "- Large Language Models (LLMs): enable machines to better understand and generate human-like language\n",
    "- intuition into the field of LLMs\n",
    "- field is very fast, can lead to frustration\n",
    "- focus on fundamentals of LLMs\n",
    "- distinction representation models (RM) vs. generative models (GM): \n",
    "    - RM do not generate text, used for task-specific cases, e.g. classification \n",
    "    - GM: generate texts, like GPT models\n",
    "- \"large\" in large language models: used losely, size descriptions are often rather arbitrary and not always indicative of capability\n",
    "\n",
    "## Prerequisites\n",
    "- some experience in Python\n",
    "- familiar with fundamentals of machine learning\n",
    "\n",
    "## Book structure\n",
    "\n",
    "### Part I: Understanding LLMs\n",
    "Inner workings of LLMs, incl. tokenization, embeddings.\n",
    "\n",
    "### Part II: Using LLMs\n",
    "LLMs in common use cases: \n",
    "- supervised classification\n",
    "- text clustering and topic modeling\n",
    "- semantic search\n",
    "- generating text\n",
    "- visual domain\n",
    "\n",
    "### Part III: Training and Fine-Tuning Language Models\n",
    "Advanced concepts through training and fine-tuning. \n",
    "\n",
    "## Hardware and Software Requirements\n",
    "- GPU (e.g. via Google Colab or locally)\n",
    "- code and tutorials available on GitHub: https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\n",
    "- API Keys (free tier, with rate limits): OpenAI, Cohere, HuggingFace \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
